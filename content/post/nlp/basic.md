---
date: 2018-05-17
title: "NLP一些经典算法"
tags:
    - nlp
    - algorithm
author:     "Chanrom"
catalog:    true
comment: true
mathjax : ture
---

# TF-IDF

## 简介

```TF-IDF```，指的是词频-逆文档频率（```Term Frequency-Inverse Document Frequency```），一种用于文档检索的加权计算方法，常用于文档的向量表示。```TF-IDF```可表示某个词在当前文档的重要程度，具体地，词的重要性和它在文档中出现的次数成正比，但是和它在整个语料库出现的次数成反比。通俗的说就是，一个词语在一篇文档中出现的次数越多而在所有文档出现的次数越少，就越能代表这个文档。

## 定义

```TF-IDF```的定义如下：
$$
TFIDF\_{i,j} = \frac{N\_{i,j}}{N\_{\cdot j}} \log (\frac{D}{D\_i})
$$
其中\\( N\_{i,j} \\) 表示词\\( i \\) 出现在文档\\( j \\)的次数；

\\( N\_{\cdot, j} \\) 表示文档\\( j \\)一共有多少词；

\\( D \\) 表示文档的总数；

\\( D\_{i} \\) 表示有多少文档出现过词\\( i \\)。

注意，以上提到的词一般先经过了无用词（```stopwords```）的过滤。


# LSA

## 简介

```LSA```（潜在语义分析）目的是解决通过搜索词（```search words```）定位出相关的文档的问题。```LSA```的基本原理是将文档和单词都映射到同一个语义空间（```concept space```）中，并在该空间进行对比分析。

## 定义

首先需要构建词-文档的矩阵 \\(A\\)，\\(A\\) 中的每个元素 \\(A\_{i j}\\) 表示搜索词 \\(w\_i\\) 在文档 \\(d\_j\\) 出现的次数（一般又转换为```TF-IDF```并作平滑处理）。

接着对 \\(A\\) 做奇异值分解：
$$
A = U S V^T
$$
\\(U\\) 为一个 \\(R^{t \times n}\\) 的正交矩阵，每行代表一个词的表示，其含义为该词在语义空间的每个维度上中重要程度；

\\(V\\) 为一个 \\(R^{d \times n}\\) 的正交矩阵，每行代表一个文档的表示，其含义为该文档在语义空间的每个维度上中重要程度；

\\(S\\) 为一个 \\(R^{n \times n}\\) 的奇异矩阵，每个元素刻画了词和文档映射到语义空间后二者的相关度。

## 优缺点

优点：

- 低维的向量表示可以缓解多词一义的情况（一词多义应该也可以有一点的作用，只是很有限）
- 降维可以去除部分噪音，特征的表示更加鲁棒

缺点：

- 貌似因为一个词就是一个表示，不能很好的处理一词多义的情况
- SVD分解的复杂度很高
- 物理解释偏弱


# pLSA

## 介绍

pLSA是在LSA的基础上提出的，一般就可以称之为主题模型。pLSA在模型中引入了一个隐变量：主题变量z，进一步刻画了文档生成的过程。在介绍pLSA之前，先定义基本的符号：

- \\( w \\) 表示词，\\( V\\) 表示所有单词的个数，也就是词表大小（在语料库中统计，固定值）
- \\( z\\) 表示主题，\\( K\\) 代表主题的个数（预先设定，固定值）
- \\( D = (d\_1, d\_2, ..., d\_M)\\)，其中 \\( M\\) 代表语料库中文档数目
- \\( d = (w\_1, w\_2, ..., w\_N)\\)，其中 \\( N\\) 代表该文档内的词的数目

pLSA定义了语料库的生成过程（如图所示）：

- 对于每一篇文档 \\( d \\)
  - 对于该文档的每个位置
  - 选择一个主题 \\( z \\) 
  - 根据主题选择词 \\( w\\)

<p align="center"> 
<img src="/pLSA.jpeg">
</p>


## 定义

在形式化定义如上的生成过程之前，我们先定义一些变量：

- \\( p(d\_i) \\) 表示语料库中某篇文档被选中的概率
- \\( p(w\_j | d\_i)\\) 表示给定文档 \\( d\_i\\) 时，词 \\( w\_j\\) 出现的概率
- \\( p(z\_k | d\_i)\\) 表示给定文档 \\( d\_i\\) 时，主题 \\( z\_k\\) 出现的概率
- \\( p(w\_j | z\_k)\\) 表示主题 \\( z\_k\\) 时，词 \\( w\_j\\) 出现的概率

重写以上的生成过程：

1. 按照概率 \\( p(d) \\) 选择一篇文档 \\( d\_i \\) （一般计为等概率的）
2. 按照文档-主题分布 \\( p(z | d\_i)\\) 选择某个主题 \\( z\_k \\)
3. 按照主题-词分布 \\( p(w | z\_k)\\) 选择某个词 \\( w\_j \\)
4. 根据 \\( M \\) 和 \\( N \\) 重复以上过程


## 求解

上一小节我们描述了pLSA的生成过程。通常来说我们拿到的仅仅只有语料库，我们更希望获得的是模型潜在的主题信息，这样就可以使用主题信息去刻画文本和词的关系。由以上公式的定义可知，\\( p(w\_j | d\_i)\\) 是已知的（参看TF-IDF的计算方法），我们可以根据大量的已知的文档-词的信息推出文档-主题以及主题-词的信息：

$$
p(w\_j| d\_i) = \sum\_{k=1}^K p(w\_j|z\_k) p(z\_k|d\_i)
$$

因此某文档中某个词的生成概率为:

$$
p(w\_j, d\_i) = p(d\_i) p(w\_j| d\_i)\\
 = p(d\_i) \sum\_{k=1}^K p(w\_j|z\_k) p(z\_k|d\_i)
$$
\\(p(d\_i)\\)可以事先计算出来，而 \\( p(w\_j|z\_k)\\) 和 \\( p(z\_k|d\_i)\\) 是未知的。因此为了得到主题的信息，必须估计 \\( p(w\_j|z\_k)\\) 和 \\( p(z\_k|d\_i)\\) 的参数。因为模型含有隐变量，考虑使用EM算法。


## EM参数估计

在介绍EM参数估计之前，我们先列出模型的参数并一一解释。

- 假定 \\( p(w\_j|z\_k)\\) 服从多项式分布，该分布的参数 \\( \phi\_k \\) 表示的是主题 \\( z\_k \\) 在词表（一共\\( V \\) 个词）上面的一个“分布”，即每个词 \\( w\_j \\) 在该参数里都对应一个元素 \\( \phi\_{k, j} \\)，代表了 \\( w\_j \\) 出现在主题 \\( z\_k \\) 上的概率：

$$
 p(w\_j|z\_k)= \phi\_{k, j} ，\sum\_{j = 1}^V \phi\_{k, j} = 1
$$

- 假定 \\( p(z\_k|d\_i)\\) 服从多项式分布，该分布的参数 \\( \theta\_i \\) 表示的是文本 \\( d\_i \\) 在主题（一共 \\( K \\) 个主题上面的一个“分布”，即每个主题 \\( z\_k \\) 在该参数里都对应一个元素 \\( \theta\_{i, k} \\)，代表了主题 \\( z\_k \\) 出现在文档 \\( d\_i \\) 上的概率：

$$
 p(z\_k|d\_i)= \theta\_{i, k} ，\sum\_{k = 1}^K \theta\_{i, k} = 1
$$

因此我们要最终求解的两个参数矩阵：

$$
\Phi = [\phi\_1, ..., \phi\_K]
$$

$$
\Theta = [\theta\_1, .., \theta\_M]
$$

由于词和词的生成是独立的，文档和文档的生成也是独立的，因此整个语料库的生成词的概率为：

$$
p(W | D) = \prod\_{i=1}^M \prod_{j=1}^N p(w\_j, d\_i) ^{n(d\_i, w\_j)}
$$
其中，\\( {n(d\_i, w\_j)} \\) 表示词 \\( w\_j \\) 在文档 \\( d\_i \\) 出现的次数。于是我们可以得到对数似然函数：

$$
l(\Phi, \Theta) = \sum\_{i=1}^M \sum\_{j=1}^n n(d\_i, w\_j) \log p(w\_j, d\_i)
$$
$$
~~~~~~~~~~~~~~ = \sum\_{i=1}^M \sum\_{j=1}^n n(d\_i, w\_j) \log p(w\_j, d\_i)
$$

[继续请参看](https://blog.csdn.net/pipisorry/article/details/42560693)

[2](https://people.cs.pitt.edu/~milos/courses/cs3750/lectures/class11.pdf)

[3](http://vdisk.weibo.com/s/bjfcErv7QQc)


