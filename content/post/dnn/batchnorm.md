---
date: 2018-05-10
title: "Batch Normalization"
tags:
    - dnn
author:     "Chanrom"
catalog:    true
comment: true
mathjax : ture
---

# 介绍

```Batch Normalization```是很多深层网络会使用的一种训练```trick```，这篇博客主要想从谷歌的原始论文和一些大牛的博客出发，重新阐述一下这个训练深层网络的“大杀器”。

# The Insights

我们知道深度学习在图像、语言和文本等领域都取得了很大的进步。在这当中，随机梯度下降（```SGD```）作为一种有效的训练深层网络的方法而被很多人所熟知。在```SGD```的基础之上的一些改进优化算法，如```Adam```、```Adagrad```和```NAdam```等通常都能将深层网络训练到最佳的性能。


举个例子，假设我们要使用```SGD```来优化一个含有参数 \\( \Theta \\) 的网络：
$$
\Theta = argmin\_{\Theta} \frac{1}{N} \sum\_{i=1}^N l(x\_i, \Theta),
$$
每个```step```使用梯度下降来更新一次 \\( \Theta \\)，最后得到最优的值。每个```step```可以使用整个数据集来计算一次梯度，也可以使用一个样本来计算一次梯度，但这两种方法都有各自的缺点，所以一般使用的是批处理（```mini-batch```）随机梯度下降。**其优点在于**：1) \\( m \\) 个样本的梯度近似于整个数据集的梯度，相对来说梯度比较稳定，并且随着\\( m \\) 的增大，这种稳定性更好；2) 相比于利用单个样本去计算梯度，批处理的方式更高效。数学上看，```mini-batch```的方式每次使用 \\( m \\) 个样本计算一次梯度，其梯度为：
$$
\frac{1}{m} \frac{\partial l(x\_i, \Theta)}{\partial \Theta}.
$$


虽然随机梯度下降的方法简单高效，使得训练很深的网络也不在话下，但是想要使得深层网络获得更佳的性能却仍然不是一件容易的事情。实际上，想要深层网络训练的好，模型超参数、学习速率和模型参数的初始化方法的选择都至关重要。这些因素在网络变得更深的时候显得格外重要，因为底层网络参数一丁点的改变都将影响高层网络的输入。


在领域适应（```domain adaptation```）的研究当中，一个本质的问题就是某个学习系统的输入分布不是固定不变的（如训练样本的分布和测试样本的分布不一致会导致分类器的性能降低），这种情况称之为```covariate shift```。```Covariate shift```导致学习系统需要有额外的能力去适应输入分布的变化。结合之前提到的深层网络的训练问题，如果我们把一个深层网络的每一层全部分开来看，```covariate shift```的问题同样存在。假设某个网络有这样一个计算过程：
$$
l = F\_2(F\_1(u, \Theta\_1), \Theta\_2),
$$
\\( F \\) 指任意的函数变换。当我们学习参数\\( \Theta\_2\\) 的时候，可以将 \\( F\_2 \\) 的输入看做是 \\( v = F\_1(u, \Theta\_1) \\)：
$$
l = F\_2(v, \Theta).
$$
这样，一次梯度下降可以用如下计算方式：
$$
\Theta\_2 \leftarrow \Theta\_2 - \frac{\alpha}{m} \sum\_{i = 1}^m \frac{\partial F\_2(v\_i, \Theta\_2)}{\partial \Theta\_2}
$$
所以从上面的角度看待深层网络的训练时我们发现，如果输入 \\( v \\) 的分布是固定不变的，那么训练含有参数 \\(\Theta\_2 \\) 的高层网络就会变得更加容易，也能训练的更好：在训练的过程中，\\(\Theta\_2 \\) 不需要调整自己去“补偿”因为输入分布变化所带来的不利影响。


前面我们了解到深度网络某一层的输入保持固定不变的分布形式对该层的训练是非常有利的，实际上输入保持固定不变的分布还能为除该层之外的子网络带来益处。考虑深层网络的某一层使用的是```sigmoid```激活函数：
$$
z = g(W v + b),
$$
其中 \\( v \\) 是该层的输入，其他为参数。\\(g \\) 是```sigmoid```函数。随着 \\(|x|\\) 的增大，\\( g^\prime(x) \\) 都是趋近于0的。这就意味着如果\\( x = W v + b \\) 中某一维度的绝对值如果不能比较小的话，能够传给输入 \\( v \\) 的梯度就会```vanish```，进而训练就会变得很慢。但是呢，因为深层网络层数很多，这往往是不可避免的：在不断训练的过程中，梯度要么很大要么很小，所以到头来 \\(x \\) 还是慢慢进入到非线性函数的```saturated regime```，使得几乎没有梯度能够往底层的网络传播（```ReLU```）的设计就是为了缓解非线性激活函数梯度消失的这个问题）。那如果我们使得输入始终稳定为一个较稳定的分布，那么是不是有可能在训练的过程中降低激活函数陷入```saturated regime```的可能性，使得训练加快呢？


基于以上两点```insights```，一种可以消除深层网络里```Internal Covatiate Shift```的机制被提出，那就是```Batch Normalization```。```Batch normalization```不仅可以加速网络的训练，而且降低了梯度对量级很小的网络参数以及参数初始化的敏感度。有了```Batch normalization```之后，我们可以为网络设定一个更高的学习速率。


# Internal Covariate Shift












